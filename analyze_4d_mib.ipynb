{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# MIB File Analysis for 4D STEM Data\n",
    "\n",
    "This notebook analyzes MIB files to understand their structure, particularly for 4D STEM datasets.\n",
    "\n",
    "## Overview\n",
    "- MIB files contain header information followed by frame data\n",
    "- Each frame has its own header + detector data\n",
    "- For 4D STEM: frames correspond to scan positions, detector data is reciprocal space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Import our MIB parsing functions\nfrom src.mib_viewer.io.mib_loader import get_mib_properties, MibProperties"
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Define Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mib_header(filepath):\n",
    "    \"\"\"Analyze MIB file header and extract key information\"\"\"\n",
    "    print(f'Analyzing: {os.path.basename(filepath)}')\n",
    "    print(f'File size: {os.path.getsize(filepath) / (1024**3):.2f} GB\\n')\n",
    "    \n",
    "    # Read the main header (first 384 bytes)\n",
    "    with open(filepath, 'rb') as f:\n",
    "        header_bytes = f.read(384)\n",
    "        f.seek(0, os.SEEK_END)\n",
    "        filesize = f.tell()\n",
    "    \n",
    "    # Parse header as comma-separated values\n",
    "    header_fields = header_bytes.decode().split(',')\n",
    "    \n",
    "    print('Header Fields (first 15):')\n",
    "    for i, field in enumerate(header_fields[:15]):\n",
    "        print(f'  [{i:2d}]: {field}')\n",
    "    \n",
    "    return header_fields, filesize\n",
    "\n",
    "def analyze_frame_structure(header_fields, filesize):\n",
    "    \"\"\"Analyze frame structure and calculate dimensions\"\"\"\n",
    "    # Parse header with existing function\n",
    "    props = get_mib_properties(header_fields)\n",
    "    \n",
    "    print(f'\\n=== Detector Properties ===')\n",
    "    print(f'Detector size: {props.merlin_size[0]} x {props.merlin_size[1]} pixels')\n",
    "    print(f'Pixel data type: {props.pixeltype}')\n",
    "    print(f'Frame header size: {props.headsize} bytes')\n",
    "    print(f'Dynamic range: {props.dyn_range}')\n",
    "    print(f'Mode: {\"Single\" if props.single else \"Quad\"}')\n",
    "    \n",
    "    # Calculate frame structure\n",
    "    detector_pixels = props.merlin_size[0] * props.merlin_size[1]\n",
    "    pixel_size = props.pixeltype.itemsize\n",
    "    data_size_per_frame = detector_pixels * pixel_size\n",
    "    frame_size = props.headsize + data_size_per_frame\n",
    "    \n",
    "    print(f'\\n=== Frame Structure ===')\n",
    "    print(f'Pixels per frame: {detector_pixels:,}')\n",
    "    print(f'Bytes per pixel: {pixel_size}')\n",
    "    print(f'Data bytes per frame: {data_size_per_frame:,}')\n",
    "    print(f'Total bytes per frame: {frame_size:,} (header + data)')\n",
    "    \n",
    "    # Calculate number of frames\n",
    "    num_frames = filesize // frame_size\n",
    "    \n",
    "    print(f'\\n=== Dataset Structure ===')\n",
    "    print(f'Total frames: {num_frames:,}')\n",
    "    \n",
    "    return props, num_frames\n",
    "\n",
    "def guess_scan_dimensions(num_frames):\n",
    "    \"\"\"Guess the most likely scan dimensions\"\"\"\n",
    "    print(f'\\n=== Scan Dimension Analysis ===')\n",
    "    \n",
    "    # Check for perfect square\n",
    "    sqrt_frames = int(np.sqrt(num_frames))\n",
    "    if sqrt_frames * sqrt_frames == num_frames:\n",
    "        print(f'Perfect square: {sqrt_frames} x {sqrt_frames}')\n",
    "        return (sqrt_frames, sqrt_frames)\n",
    "    \n",
    "    # Find all factor pairs\n",
    "    factors = []\n",
    "    for i in range(1, int(np.sqrt(num_frames)) + 1):\n",
    "        if num_frames % i == 0:\n",
    "            factors.append((i, num_frames // i))\n",
    "    \n",
    "    print('Possible rectangular scans:')\n",
    "    for w, h in factors[-10:]:  # Show last 10 (largest factors)\n",
    "        ratio = max(w, h) / min(w, h)\n",
    "        print(f'  {w:3d} x {h:3d} (aspect ratio: {ratio:.2f})')\n",
    "    \n",
    "    # Find most square-like\n",
    "    best_ratio = float('inf')\n",
    "    best_dims = factors[-1]\n",
    "    for w, h in factors:\n",
    "        ratio = max(w, h) / min(w, h)\n",
    "        if ratio < best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_dims = (w, h)\n",
    "    \n",
    "    print(f'\\nMost square-like: {best_dims[0]} x {best_dims[1]} (ratio: {best_ratio:.2f})')\n",
    "    return best_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Analyze 4D STEM Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "data_dir = Path('/media/o2d/data/ORNL Dropbox/Ondrej Dyck/TEM data/2025/Andy 4D')\n",
    "files = {\n",
    "    'large': data_dir / '1_256x256_2msec_graphene.mib',\n",
    "    'small': data_dir / '64x64 Test.mib'\n",
    "}\n",
    "\n",
    "# Check files exist\n",
    "for name, path in files.items():\n",
    "    if path.exists():\n",
    "        print(f'{name}: {path.name} ✓')\n",
    "    else:\n",
    "        print(f'{name}: {path.name} ✗ (not found)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Large Dataset Analysis (256x256 scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('LARGE DATASET ANALYSIS')\n",
    "print('=' * 60)\n",
    "\n",
    "if files['large'].exists():\n",
    "    header_fields, filesize = analyze_mib_header(files['large'])\n",
    "    props, num_frames = analyze_frame_structure(header_fields, filesize)\n",
    "    scan_dims = guess_scan_dimensions(num_frames)\n",
    "    \n",
    "    # Calculate 4D array shape\n",
    "    print(f'\\n=== 4D Array Properties ===')\n",
    "    shape_4d = (scan_dims[1], scan_dims[0], props.merlin_size[1], props.merlin_size[0])\n",
    "    print(f'4D shape (sy, sx, qy, qx): {shape_4d}')\n",
    "    \n",
    "    # Memory requirements\n",
    "    total_elements = np.prod(shape_4d)\n",
    "    bytes_per_element = props.pixeltype.itemsize\n",
    "    memory_gb = (total_elements * bytes_per_element) / (1024**3)\n",
    "    print(f'Total data points: {total_elements:,}')\n",
    "    print(f'Memory requirement: {memory_gb:.2f} GB')\n",
    "else:\n",
    "    print('Large file not found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Small Dataset Analysis (64x64 scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 60)\n",
    "print('SMALL DATASET ANALYSIS')\n",
    "print('=' * 60)\n",
    "\n",
    "if files['small'].exists():\n",
    "    header_fields, filesize = analyze_mib_header(files['small'])\n",
    "    props, num_frames = analyze_frame_structure(header_fields, filesize)\n",
    "    scan_dims = guess_scan_dimensions(num_frames)\n",
    "    \n",
    "    # Calculate 4D array shape\n",
    "    print(f'\\n=== 4D Array Properties ===')\n",
    "    shape_4d = (scan_dims[1], scan_dims[0], props.merlin_size[1], props.merlin_size[0])\n",
    "    print(f'4D shape (sy, sx, qy, qx): {shape_4d}')\n",
    "    \n",
    "    # Memory requirements\n",
    "    total_elements = np.prod(shape_4d)\n",
    "    bytes_per_element = props.pixeltype.itemsize\n",
    "    memory_gb = (total_elements * bytes_per_element) / (1024**3)\n",
    "    print(f'Total data points: {total_elements:,}')\n",
    "    print(f'Memory requirement: {memory_gb:.2f} GB')\n",
    "else:\n",
    "    print('Small file not found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Load and Visualize Sample Data\n",
    "\n",
    "Let's load a small portion of the data to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_frames(filepath, num_sample_frames=4):\n",
    "    \"\"\"Load a few sample frames to understand data structure\"\"\"\n",
    "    print(f'Loading {num_sample_frames} sample frames from {os.path.basename(filepath)}')\n",
    "    \n",
    "    # Get file properties\n",
    "    with open(filepath, 'rb') as f:\n",
    "        header_fields = f.read(384).decode().split(',')\n",
    "    \n",
    "    props = get_mib_properties(header_fields)\n",
    "    \n",
    "    # Create memory-mapped array for just the sample frames\n",
    "    merlin_frame_dtype = np.dtype([\n",
    "        ('header', np.bytes_, props.headsize),\n",
    "        ('data', props.pixeltype, props.merlin_size)\n",
    "    ])\n",
    "    \n",
    "    # Load sample frames\n",
    "    sample_data = np.memmap(\n",
    "        filepath,\n",
    "        dtype=merlin_frame_dtype,\n",
    "        mode='r',\n",
    "        shape=(num_sample_frames,)\n",
    "    )\n",
    "    \n",
    "    return sample_data, props\n",
    "\n",
    "# Load sample from small dataset (more manageable)\n",
    "if files['small'].exists():\n",
    "    sample_data, props = load_sample_frames(files['small'], 4)\n",
    "    \n",
    "    print(f'Sample data shape: {sample_data.shape}')\n",
    "    print(f'Frame data shape: {sample_data[\"data\"][0].shape}')\n",
    "    print(f'Data type: {sample_data[\"data\"].dtype}')\n",
    "    \n",
    "    # Show statistics for first frame\n",
    "    first_frame = sample_data['data'][0]\n",
    "    print(f'\\nFirst frame statistics:')\n",
    "    print(f'  Min: {first_frame.min()}')\n",
    "    print(f'  Max: {first_frame.max()}')\n",
    "    print(f'  Mean: {first_frame.mean():.2f}')\n",
    "    print(f'  Std: {first_frame.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sample frames\n",
    "if 'sample_data' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(4):\n",
    "        frame = sample_data['data'][i]\n",
    "        im = axes[i].imshow(frame, cmap='viridis')\n",
    "        axes[i].set_title(f'Frame {i} (Diffraction Pattern)')\n",
    "        axes[i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show sum of all sample frames (should show average diffraction pattern)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Sum\n",
    "    sum_pattern = np.sum(sample_data['data'], axis=0)\n",
    "    im1 = ax1.imshow(sum_pattern, cmap='viridis')\n",
    "    ax1.set_title('Sum of Sample Frames')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Log scale\n",
    "    im2 = ax2.imshow(np.log1p(sum_pattern), cmap='viridis')\n",
    "    ax2.set_title('Sum of Sample Frames (Log Scale)')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "Based on the analysis above, let's calculate the performance requirements and chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance_requirements(shape_4d, dtype):\n",
    "    \"\"\"Analyze memory and performance requirements for 4D dataset\"\"\"\n",
    "    sy, sx, qy, qx = shape_4d\n",
    "    bytes_per_element = dtype.itemsize\n",
    "    \n",
    "    print(f'4D Dataset: {sy} × {sx} × {qy} × {qx}')\n",
    "    print(f'Data type: {dtype} ({bytes_per_element} bytes/pixel)')\n",
    "    \n",
    "    # Total memory\n",
    "    total_elements = sy * sx * qy * qx\n",
    "    total_gb = (total_elements * bytes_per_element) / (1024**3)\n",
    "    print(f'\\nTotal dataset: {total_gb:.2f} GB')\n",
    "    \n",
    "    # Slice sizes\n",
    "    real_space_slice = qy * qx * bytes_per_element  # Single diffraction pattern\n",
    "    recip_space_slice = sy * sx * bytes_per_element  # Single k-space pixel\n",
    "    \n",
    "    print(f'\\nSlice sizes:')\n",
    "    print(f'  Single diffraction pattern: {real_space_slice / 1024:.1f} KB')\n",
    "    print(f'  Single reciprocal space slice: {recip_space_slice / 1024:.1f} KB')\n",
    "    \n",
    "    # Chunking recommendations\n",
    "    target_chunk_mb = 10  # Target 10MB chunks\n",
    "    elements_per_chunk = (target_chunk_mb * 1024 * 1024) // bytes_per_element\n",
    "    \n",
    "    # Real space chunking (good for virtual imaging)\n",
    "    frames_per_chunk = elements_per_chunk // (qy * qx)\n",
    "    chunk_sy = min(sy, int(np.sqrt(frames_per_chunk)))\n",
    "    chunk_sx = min(sx, frames_per_chunk // chunk_sy)\n",
    "    \n",
    "    print(f'\\nRecommended chunking for HDF5:')\n",
    "    print(f'  Target chunk size: {target_chunk_mb} MB')\n",
    "    print(f'  Real-space chunks: ({chunk_sy}, {chunk_sx}, {qy}, {qx})')\n",
    "    \n",
    "    # Memory requirements for different operations\n",
    "    print(f'\\nMemory for common operations:')\n",
    "    print(f'  Load 1 diffraction pattern: {real_space_slice / 1024:.1f} KB')\n",
    "    print(f'  Load 1 real-space line: {sx * qy * qx * bytes_per_element / (1024**2):.1f} MB')\n",
    "    print(f'  Load 1 recip-space line: {sy * sx * bytes_per_element / 1024:.1f} KB')\n",
    "    print(f'  Virtual dark field image: {sy * sx * bytes_per_element / 1024:.1f} KB')\n",
    "\n",
    "# Analyze both datasets if available\n",
    "if 'props' in locals():\n",
    "    print('SMALL DATASET PERFORMANCE:')\n",
    "    analyze_performance_requirements((64, 64, props.merlin_size[1], props.merlin_size[0]), props.pixeltype)\n",
    "    \n",
    "    print('\\n' + '='*50)\n",
    "    print('LARGE DATASET PERFORMANCE:')\n",
    "    analyze_performance_requirements((256, 256, props.merlin_size[1], props.merlin_size[0]), props.pixeltype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Based on this analysis, we can now plan the 4D viewer implementation:\n",
    "\n",
    "1. **Data Structure**: 4D arrays with shape (scan_y, scan_x, detector_y, detector_x)\n",
    "2. **Memory Management**: Use HDF5 with chunking for datasets > 4GB\n",
    "3. **Visualization**: Multiple linked 2D views with real-time slicing\n",
    "4. **Performance**: PyQtGraph for real-time updates, downsampling during interaction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}